{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a Perturbation\n",
    "\n",
    "Before performing a perturbation on the network we prepare the graph the following way:\n",
    "- Constructing Randomized Inward-Links for Fake Basal Species\n",
    "- Pruning the Network Based on Optimal Foraging Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Randomized Inward-Links for Fake Basal Species\n",
    "\n",
    "The following process aims to create randomized inward-links for species which, due to missing data, falsely appear to be basal species. While these species do not hold a true basal role, they lack inward-edges in the network representation, thus misleadingly presenting them as basal species. \n",
    "\n",
    "The newly created links will provide a more realistic portrayal of these species' interactions within the ecosystem, based on their diet, habitat, and zone attributes.\n",
    "\n",
    "Below is a detailed breakdown of this procedure:\n",
    "\n",
    "1. **Link Assignment**: Starting with the dataset titled \"species_for_randomized_link_assignment\", we examine each row to extract the species (denoted as a 'taxon'), its diet, diet rank, habitat, and zone.\n",
    "\n",
    "2. **Linking to Compatible Species**: We then refer to a secondary dataset to identify species that could potentially interact with our fake basal species. In this dataset, we find rows where the diet rank from the first dataset matches a column name and the habitat is the same. We connect the fake basal species to all such rows, with each connection representing a possible interaction link based on shared diet preferences and habitats.\n",
    "\n",
    "3. **Removing Duplicate Links**: Following this, we may have duplicate links representing the same interactions. To address this, we amalgamate all identical links and remove duplicates.\n",
    "\n",
    "4. **Sampling Links**: For each fake basal species, we retain a subset of potential links, specifically, 5% of these links (rounded up).\n",
    "\n",
    "5. **Integration into the MetaWeb**: Finally, these selected links are added to our overarching ecosystem network, referred to as the 'MetaWeb'.\n",
    "\n",
    "The number of links we retain varies depending on the species' dietary range:\n",
    "\n",
    "- For species with a **Generalized** diet, we retain 5% of the potential interaction links.\n",
    "- For species with a **Specialized** diet, we randomly select between 1 and 5 potential interaction links.\n",
    "\n",
    "This ensures that our network adequately represents the varying interaction potential of species based on their diet specialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe_from_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def explode_multivalue_columns(df, columns_to_explode):\n",
    "    for column in columns_to_explode:\n",
    "        df[column] = df[column].str.split('; ')\n",
    "        df = df.explode(column)\n",
    "    return df\n",
    "\n",
    "def filter_subdiet_by_habitat_and_zone(nodes_df, species_info):\n",
    "    sub_diet = nodes_df[nodes_df[species_info['Diet_Rank']] == species_info['Diet_Name']]\n",
    "    return sub_diet[(sub_diet['Habitat'] == species_info['Habitat']) & (sub_diet['Zone'] == species_info['Zone'])]\n",
    "\n",
    "def construct_linkage_dataframe(species_info, sub_diet_filtered):\n",
    "    linkage_dataframe = pd.DataFrame(columns=['Diet_Range', 'Source', 'Target'])\n",
    "    number_of_rows = len(sub_diet_filtered)\n",
    "    linkage_dataframe['Source'] = np.full((number_of_rows), species_info['Taxon'])\n",
    "    linkage_dataframe['Diet_Range'] = np.full((number_of_rows), species_info['Diet_Range'])\n",
    "    linkage_dataframe['Target'] = sub_diet_filtered['Taxon'].values\n",
    "    return linkage_dataframe\n",
    "\n",
    "def sample_rows_based_on_diet_range(species_group):\n",
    "    if species_group['Diet_Range'].iloc[0] == 'Generalised':\n",
    "        return species_group.head(int(np.ceil(0.05*len(species_group))))\n",
    "    else: # assuming other category is 'Specialised'\n",
    "        return species_group.sample(min(len(species_group), random.randint(1, 5)))\n",
    "\n",
    "def generate_links(nodes_df, random_species_links_df):\n",
    "    all_possible_links = pd.DataFrame(columns=['Diet_Range', 'Source', 'Target'])\n",
    "    \n",
    "    for i in range(len(random_species_links_df)):\n",
    "        species_info = random_species_links_df.iloc[i]\n",
    "        sub_diet_filtered = filter_subdiet_by_habitat_and_zone(nodes_df, species_info)\n",
    "        linkage_dataframe = construct_linkage_dataframe(species_info, sub_diet_filtered)\n",
    "        all_possible_links = pd.concat([all_possible_links, linkage_dataframe])\n",
    "\n",
    "    all_possible_links = all_possible_links.drop_duplicates(subset=['Source', 'Target'])\n",
    "    return all_possible_links.groupby('Source').apply(sample_rows_based_on_diet_range).reset_index(drop=True)\n",
    "\n",
    "node_list_dataframe = load_dataframe_from_csv('../node_lists/all_species_and_feeding_groups.csv')\n",
    "node_list_dataframe = explode_multivalue_columns(node_list_dataframe, ['Habitat', 'Zone'])\n",
    "\n",
    "random_species_links_dataframe = load_dataframe_from_csv('../node_lists/species_for_randomized_links.csv')\n",
    "random_species_links_dataframe = explode_multivalue_columns(random_species_links_dataframe, ['Habitat', 'Zone'])\n",
    "\n",
    "sampled_links = generate_links(node_list_dataframe, random_species_links_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges to G\n",
    "G = \"Network\"\n",
    "for i in range(len(sampled_links)):\n",
    "    edge = sampled_links[i]\n",
    "    source = edge['Source']\n",
    "    target = edge['Target']\n",
    "    G.add_edge(source, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning the Network Based on Optimal Foraging Theory\n",
    "\n",
    "Because of the expantion of the dataset, the food web actually represents a meta web, which is a potential food web. \n",
    "Therefore, before performing a perturbation on the network we remove some links.\n",
    "\n",
    "Here's the step-by-step process:\n",
    "\n",
    "1. **Classify generalists and specialists**: A threshold value of in-degree, `k`, is used for this classification. If a species (node in the network) has an in-degree greater than or equal to `k`, it is considered a generalist; otherwise, it is considered a specialist.\n",
    "\n",
    "2. **Preserve a fraction of the links for generalists**: For each generalist, we randomly keep only 10% of its inward edges (links from other species to this one, signifying a predation relationship), effectively removing 90% of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(G, k_threshhold, percentage_to_remove=0.9):\n",
    "    nodes = list(G.nodes)\n",
    "    for node in nodes:\n",
    "        in_degree = G.in_degree(node)\n",
    "\n",
    "        if in_degree >= k_threshhold:\n",
    "            inward_edges = list(G.in_edges(node))\n",
    "            num_edges_to_remove = int(in_degree * percentage_to_remove)\n",
    "            edges_to_remove = random.sample(inward_edges, num_edges_to_remove)\n",
    "            G.remove_edges_from(edges_to_remove)\n",
    "    return G\n",
    "\n",
    "# usage\n",
    "threshold_k = 10\n",
    "G = remove_links(G, threshold_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodweb_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
